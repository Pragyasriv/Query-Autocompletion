{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmkGV0IRFlC1",
        "outputId": "8af35f70-cb18-484d-e05e-3f05b4b1e56a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.11.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "!pip install python-docx\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import statements\n",
        "import docx\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import pandas as pd\n",
        "from docx import Document"
      ],
      "metadata": {
        "id": "_WChS_Y4DQXD"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the document\n",
        "file_path = \"/content/wikipedia.docx\"\n",
        "\n",
        "doc = docx.Document(file_path)\n",
        "\n",
        "\n",
        "# Read the DOCX file and extract text\n",
        "def read_docx(doc):\n",
        "    text = \"\"\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text += paragraph.text + \" \"\n",
        "    return text\n",
        "text = read_docx(doc)\n"
      ],
      "metadata": {
        "id": "lB1hW6eNCs1q"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model: Sentence Completion\n",
        "### Preprocessing for LSTM"
      ],
      "metadata": {
        "id": "aq4xUMWPDm1E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVQqte9DF8fP",
        "outputId": "78969f5c-eb2b-4ea2-ce64-8158c067bdc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text data cleaned and saved to: output.csv\n"
          ]
        }
      ],
      "source": [
        "#Preprocessing for LSTM\n",
        "\n",
        "text_data = [paragraph.text for paragraph in doc.paragraphs]  # Extract text from paragraphs\n",
        "text_data = [text.lower() for text in text_data] # Convert text to lowercase\n",
        "text_data = [re.sub(r\"\\[.*?\\]\", \"\", text) for text in text_data] # Remove special characters and words between them using regex\n",
        "english_alphabet = set(string.ascii_lowercase) # Remove words not in the English alphabet\n",
        "text_data = [' '.join([word for word in text.split() if all(char in english_alphabet for char in word)]) for text in text_data]\n",
        "text_data = [text.strip() for text in text_data if text.strip()] # Remove leading/trailing whitespaces\n",
        "\n",
        "# Create a DataFrame with the cleaned text data\n",
        "df = pd.DataFrame({\"Text\": text_data})\n",
        "output_path = \"output.csv\"  # Save the cleaned text data to a CSV file\n",
        "df.to_csv(output_path, index=False) # Set index=False to exclude the index column in the output\n",
        "\n",
        "print(\"Text data cleaned and saved to:\", output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "iUwJjokAF_ut"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.words = self.load_words()\n",
        "        self.unique_words = self.get_unique_words()\n",
        "\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.unique_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.unique_words)}\n",
        "\n",
        "        self.word_indexes = [self.word_to_index[w] for w in self.words]\n",
        "\n",
        "    def load_words(self):\n",
        "        train_df = pd.read_csv('/content/output.csv')\n",
        "        text = train_df['Text'].str.cat(sep=' ')\n",
        "        return text.split(' ')\n",
        "\n",
        "    def get_unique_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts.keys(), key=word_counts.get, reverse=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word_indexes) - self.args\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.word_indexes[index:index + self.args]),\n",
        "            torch.tensor(self.word_indexes[index + 1:index + self.args + 1])\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building the LSTM Model"
      ],
      "metadata": {
        "id": "xcmRx7xBEe42"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "fYquILoUGbbK"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm_size = 128\n",
        "        self.embedding_dim = 128\n",
        "        self.num_layers = 3\n",
        "\n",
        "        n_vocab = len(dataset.unique_words)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (\n",
        "            torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
        "            torch.zeros(self.num_layers, sequence_length, self.lstm_size)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GwcbkVaGf6z",
        "outputId": "e04a39cd-0146-4f3d-98c5-7eae3a7eb711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Average Loss: 6.8169\n",
            "Epoch[1/5], Validation Loss:  6.6144\n",
            "Epoch [2/5], Average Loss: 6.4623\n",
            "Epoch[2/5], Validation Loss:  6.3164\n",
            "Epoch [3/5], Average Loss: 6.1956\n",
            "Epoch[3/5], Validation Loss:  6.1031\n",
            "Epoch [4/5], Average Loss: 6.0412\n",
            "Epoch[4/5], Validation Loss:  5.9756\n",
            "Epoch [5/5], Average Loss: 5.9224\n",
            "Epoch[5/5], Validation Loss:  5.8620\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Hyperparameters\n",
        "sequence_length = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "\n",
        "# Create the dataset\n",
        "dataset = TextDataset(sequence_length)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "# Create the model\n",
        "model = LSTMModel(dataset)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        inputs, targets = batch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        hidden = model.init_state(sequence_length)\n",
        "        outputs, _ = model(inputs, hidden)\n",
        "\n",
        "        loss = criterion(outputs.view(-1, len(dataset.unique_words)), targets.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Print the epoch and average loss\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "\n",
        "            hidden = model.init_state(sequence_length)\n",
        "            outputs, _ = model(inputs, hidden)\n",
        "\n",
        "            loss = criterion(outputs.view(-1, len(dataset.unique_words)), targets.view(-1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Calculate average validation loss for the epoch\n",
        "    average_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    # Print the epoch and average validation loss\n",
        "    print(f\"Epoch[{epoch+1}/{num_epochs}], Validation Loss: {average_val_loss: .4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LSTM Model - Output Generation"
      ],
      "metadata": {
        "id": "KODVXBZdFBq4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "zEBpVPxAG1gc"
      },
      "outputs": [],
      "source": [
        "def lstmmodel(input_sentence):\n",
        "  try:\n",
        "    input_indexes = [dataset.word_to_index[word] for\n",
        "                    word in input_sentence.split()]\n",
        "    input_tensor = torch.tensor(input_indexes,\n",
        "                                dtype=torch.long).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden = model.init_state(len(input_indexes))\n",
        "    outputs, _ = model(input_tensor, hidden)\n",
        "    predicted_index = torch.argmax(outputs[0, -1, :]).item()\n",
        "    predicted_word = dataset.index_to_word[predicted_index]\n",
        "    return predicted_word\n",
        "\n",
        "  except KeyError:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TESTING!!!!!!"
      ],
      "metadata": {
        "id": "GjRfMwI8FNBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  input_sentence=\"he was in\"\n",
        "  # Preprocess the input sentence\n",
        "  input_indexes = [dataset.word_to_index[word] for word in input_sentence.split()]\n",
        "  input_tensor = torch.tensor(input_indexes,dtype=torch.long).unsqueeze(0)\n",
        "      # Generate the next word\n",
        "  model.eval()\n",
        "  hidden = model.init_state(len(input_indexes))\n",
        "  outputs, _ = model(input_tensor, hidden)\n",
        "  predicted_index = torch.argmax(outputs[0, -1, :]).item()\n",
        "  predicted_word = dataset.index_to_word[predicted_index]\n",
        "\n",
        "      # Print the predicted word\n",
        "  print(\"Input Sentence:\", input_sentence)\n",
        "  print(\"Predicted Next Word:\", predicted_word)\n",
        "\n",
        "  #return predicted_word\n",
        "except KeyError:\n",
        "      # Return None if there is no suitable prediction\n",
        "      print(\"None\")"
      ],
      "metadata": {
        "id": "GJMp2GPJ2KPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420f782e-6cc5-4cc6-9156-89ebab18a3fc"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence: he was in\n",
            "Predicted Next Word: the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRIE Model: Word Completion\n",
        "\n",
        "###Building the TRIE Model"
      ],
      "metadata": {
        "id": "5_C84CA3FTqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TRIE Model\n",
        "class TrieNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.is_end_of_word = False\n",
        "\n",
        "class Trie:\n",
        "    def __init__(self):\n",
        "        self.root = TrieNode()\n",
        "\n",
        "    def insert_word(self, word):\n",
        "        node = self.root\n",
        "        for char in word:\n",
        "            if char not in node.children:\n",
        "                node.children[char] = TrieNode()\n",
        "            node = node.children[char]\n",
        "        node.is_end_of_word = True\n",
        "\n",
        "    def search_word(self, prefix):\n",
        "        node = self.root\n",
        "        for char in prefix:\n",
        "            if char not in node.children:\n",
        "                return []\n",
        "            node = node.children[char]\n",
        "        return self._collect_words_with_prefix(node, prefix)\n",
        "\n",
        "    def _collect_words_with_prefix(self, node, prefix):\n",
        "        results = []\n",
        "        if node.is_end_of_word:\n",
        "            results.append(prefix)\n",
        "        for char, child_node in node.children.items():\n",
        "            results.extend(self._collect_words_with_prefix(child_node, prefix + char))\n",
        "        return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_words(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Apply preprocessing steps\n",
        "    preprocessed_words = []\n",
        "    for word in words:\n",
        "        # Convert to lowercase\n",
        "        word = word.lower()\n",
        "\n",
        "        # Remove non-alphabetic characters\n",
        "        word = re.sub(r'[^a-zA-Z]', '', word)\n",
        "\n",
        "        # Remove stop words\n",
        "        if word not in stop_words and word != \"unk\":\n",
        "            preprocessed_words.append(word)\n",
        "\n",
        "    return preprocessed_words\n",
        "\n",
        "\n",
        "def build_trie(words):\n",
        "    trie = Trie()\n",
        "    for word in words:\n",
        "        trie.insert_word(word)\n",
        "    return trie\n",
        "\n",
        "preprocessed_words = preprocess_words(text)\n",
        "trie = build_trie(preprocessed_words)"
      ],
      "metadata": {
        "id": "jPWgmo7IrfQy"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TESTING!!!!!"
      ],
      "metadata": {
        "id": "pn3rkvxeFsE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"ser\"\n",
        "word_completions = trie.search_word(prefix)\n",
        "print(\"Word Completions for prefix '{}': {}\".format(prefix, word_completions))"
      ],
      "metadata": {
        "id": "4ioqGSzXFp80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "302774a9-b686-482c-c95e-76881b19cf54"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Completions for prefix 'ser': ['series', 'serious', 'serve', 'served', 'serves', 'servant', 'servants', 'service', 'services', 'serviced', 'servicemen', 'serving', 'serb', 'serbs', 'serbia', 'sergeant']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Getting the Best Suggestions And Predictions\n",
        "\n",
        "###Sort Model Suggestions"
      ],
      "metadata": {
        "id": "E5XDbS5Ere7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def tf_idf_relevance(user_input, suggestions, corpus):\n",
        "\n",
        "  #user_input = preprocess_text(user_input)\n",
        "  #suggestions = [preprocess_text(s) for s in suggestions]\n",
        "  #corpus = [preprocess_text(doc) for doc in corpus]\n",
        "\n",
        "  vectorizer = TfidfVectorizer()\n",
        "\n",
        "  vectorizer.fit(corpus)\n",
        "\n",
        "  user_input_vector = vectorizer.transform([user_input])\n",
        "  suggestions_vectors = vectorizer.transform(suggestions)\n",
        "  corpus_vectors = vectorizer.transform(corpus)\n",
        "\n",
        "  cosine_similarities = user_input_vector.dot(suggestions_vectors.T).toarray()[0]\n",
        "\n",
        "  ranked_suggestions = sorted(zip(suggestions, cosine_similarities), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  return dict(ranked_suggestions)"
      ],
      "metadata": {
        "id": "dpoesWmPGL8b"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Combine outputs of Trie and LSTM Model"
      ],
      "metadata": {
        "id": "H4Qhk-NwaSGo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "h-_vBj40Pydp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40837f4a-126d-4f7c-cb29-d0579369e498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence: he was act\n",
            "Top Suggestions: ['of', 'action', 'actors', 'actor', 'act']\n"
          ]
        }
      ],
      "source": [
        "user_input = \"He was act\"\n",
        "user_input = user_input.lower()\n",
        "# Tokenize the user input\n",
        "tokens = user_input.split()\n",
        "\n",
        "# Extract prefix for Trie model (last word)\n",
        "trie_prefix = tokens[-1]\n",
        "\n",
        "# Extract input sequence for LSTM model (all words)\n",
        "lstm_input_sequence = \" \".join(tokens)\n",
        "\n",
        "# Generate suggestions from Trie model using prefix\n",
        "trie_suggestions = trie.search_word(trie_prefix)\n",
        "\n",
        "# Generate predictions from LSTM model using input sequence\n",
        "lstm_predictions = lstmmodel(lstm_input_sequence)\n",
        "\n",
        "#print(\"lstm_predictions\",lstm_predictions)\n",
        "#print(\"trie_suggestions\",trie_suggestions)\n",
        "\n",
        "\n",
        "\n",
        "combined_suggestions = trie_suggestions + [lstm_predictions]\n",
        "\n",
        "# Assign weights to suggestions based on source\n",
        "trie_weight = 0.5\n",
        "lstm_weight = 0.7\n",
        "\n",
        "# Calculate relevance scores based on weights\n",
        "relevance_scores = {}\n",
        "for suggestion in combined_suggestions:\n",
        "    relevance_scores[suggestion] = 0\n",
        "    if suggestion in trie_suggestions:\n",
        "        relevance_scores[suggestion] += trie_weight\n",
        "    if suggestion == lstm_predictions:\n",
        "        relevance_scores[suggestion] += lstm_weight\n",
        "\n",
        "# Get frequency of each suggestion in the dataset\n",
        "word_frequencies = {}\n",
        "for word in preprocessed_words:\n",
        "    word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
        "\n",
        "\n",
        "# Sort suggestions based on relevance and frequency\n",
        "top_suggestions = sorted(combined_suggestions, key=lambda x: (relevance_scores.get(x, 0), word_frequencies.get(x, 0)), reverse=True)[:5]\n",
        "\n",
        "train_df = pd.read_csv('/content/output.csv')\n",
        "text = train_df['Text'].str.cat(sep=' ')\n",
        "ranked = tf_idf_relevance(user_input,top_suggestions,preprocessed_words)\n",
        "# Display top suggestions to the user\n",
        "print(\"Input Sentence:\", user_input)\n",
        "print(\"Top Suggestions:\", top_suggestions)\n",
        "#print(\"Ranked:\", ranked)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TRAIL!!!!!"
      ],
      "metadata": {
        "id": "Uotz76IzcmFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def calculate_frequency(dataset):\n",
        "    # Count the occurrences of each suggestion in the dataset\n",
        "    suggestion_counts = Counter(dataset)\n",
        "    return suggestion_counts\n",
        "\n",
        "# Example dataset\n",
        "dataset = preprocessed_words\n",
        "suggestion_frequencies = calculate_frequency(dataset)\n",
        "print(suggestion_frequencies)\n",
        "\n",
        "def sort_suggestions(trie_suggestions, trie_prefix, suggestion_frequencies):\n",
        "    def suggestion_key(suggestion):\n",
        "        length_weight = 0.3\n",
        "        prefix_weight = 0.5\n",
        "        frequency_weight = 0.2\n",
        "        return (length_weight * len(suggestion) +\n",
        "                prefix_weight * (suggestion.startswith(trie_prefix)) +\n",
        "                frequency_weight * suggestion_frequencies.get(suggestion, 0))\n",
        "\n",
        "    sorted_suggestions = sorted(trie_suggestions, key=suggestion_key, reverse=True)\n",
        "    return sorted_suggestions\n",
        "\n",
        "# Example usage\n",
        "trie_prefix = \"act\"\n",
        "sorted_suggestions = sort_suggestions(trie_suggestions, trie_prefix, suggestion_frequencies)\n",
        "print(sorted_suggestions)\n"
      ],
      "metadata": {
        "id": "o1gThRd-BWxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"He was ser\"\n",
        "user_input = user_input.lower()\n",
        "# Tokenize the user input\n",
        "tokens = user_input.split()\n",
        "\n",
        "# Extract prefix for Trie model (last word)\n",
        "trie_prefix = tokens[-1]\n",
        "\n",
        "# Extract input sequence for LSTM model (all words)\n",
        "lstm_input_sequence = \" \".join(tokens)\n",
        "\n",
        "# Generate suggestions from Trie model using prefix\n",
        "trie_suggestions = trie.search_word(trie_prefix)\n",
        "print(trie_suggestions)\n"
      ],
      "metadata": {
        "id": "0YaCxxoz7oQE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}